{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RAG with Haystack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example leverages the [Docling](https://github.com/DS4SD/docling) converter\n",
    "integration for [Haystack](https://github.com/deepset-ai/haystack/), along with\n",
    "in-memory document store and retriever instances.\n",
    "\n",
    "The presented `DoclingConverter` component enables you to:\n",
    "- use various document types in your LLM applications with ease and speed, and\n",
    "- leverage Docling's rich format for advanced, document-native grounding.\n",
    "\n",
    "`DoclingConverter` supports two different export modes:\n",
    "- `ExportType.MARKDOWN`: if you want to capture each input document as a separate\n",
    "  Haystack document, or\n",
    "- `ExportType.DOC_CHUNKS` (default): if you want to have each input document chunked and\n",
    "  to then capture each individual chunk as a separate Haystack document downstream.\n",
    "\n",
    "The example allows to explore both modes via parameter `EXPORT_TYPE`; depending on the\n",
    "value set, the ingestion and RAG pipelines are then set up accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# TODO: uncomment when package available on PyPI:\n",
    "# %pip install -q --progress-bar off docling-haystack haystack-ai docling python-dotenv\n",
    "\n",
    "%pip install -q --progress-bar off haystack-ai docling python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_API_KEY\", \"\")\n",
    "PATHS = [\n",
    "    \"https://arxiv.org/pdf/2408.09869\",  # Docling Technical Report\n",
    "    # ... additional docs can be listed here\n",
    "]\n",
    "GENERATION_MODEL_ID = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "QUESTION = \"Which are the main AI models in Docling?\"\n",
    "TOP_K = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pva/work/github.com/DS4SD/docling-haystack/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 9 files: 100%|██████████| 9/9 [00:00<00:00, 97541.95it/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1041 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'writer': {'documents_written': 54}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "from docling_haystack.converter import DoclingConverter\n",
    "\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "idx_pipe = Pipeline()\n",
    "idx_pipe.add_component(\"converter\", DoclingConverter())\n",
    "idx_pipe.add_component(\"writer\", DocumentWriter(document_store=document_store))\n",
    "idx_pipe.connect(\"converter\", \"writer\")\n",
    "idx_pipe.run({\"converter\": {\"paths\": PATHS}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pva/work/github.com/DS4SD/docling-haystack/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:2232: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.builders import AnswerBuilder\n",
    "from haystack.components.builders.prompt_builder import PromptBuilder\n",
    "from haystack.components.generators import HuggingFaceAPIGenerator\n",
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
    "from haystack.utils import Secret\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "    Given these documents, answer the question.\n",
    "    Documents:\n",
    "    {% for doc in documents %}\n",
    "        {{ doc.content }}\n",
    "    {% endfor %}\n",
    "    Question: {{query}}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "rag_pipe = Pipeline()\n",
    "rag_pipe.add_component(\n",
    "    \"retriever\",\n",
    "    InMemoryBM25Retriever(document_store=document_store, top_k=TOP_K),\n",
    ")\n",
    "rag_pipe.add_component(\"prompt_builder\", PromptBuilder(template=prompt_template))\n",
    "rag_pipe.add_component(\n",
    "    \"llm\",\n",
    "    HuggingFaceAPIGenerator(\n",
    "        api_type=\"serverless_inference_api\",\n",
    "        api_params={\"model\": GENERATION_MODEL_ID},\n",
    "        token=Secret.from_token(HF_TOKEN) if HF_TOKEN else None,\n",
    "    ),\n",
    ")\n",
    "rag_pipe.add_component(\"answer_builder\", AnswerBuilder())\n",
    "rag_pipe.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "rag_pipe.connect(\"prompt_builder\", \"llm\")\n",
    "rag_pipe.connect(\"llm.replies\", \"answer_builder.replies\")\n",
    "rag_pipe.connect(\"llm.meta\", \"answer_builder.meta\")\n",
    "rag_pipe.connect(\"retriever\", \"answer_builder.documents\")\n",
    "rag_res = rag_pipe.run(\n",
    "    {\n",
    "        \"retriever\": {\"query\": QUESTION},\n",
    "        \"prompt_builder\": {\"query\": QUESTION},\n",
    "        \"answer_builder\": {\"query\": QUESTION},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Which are the main AI models in Docling?\n",
      "\n",
      "Answer:\n",
      "The main AI models in Docling are a layout analysis model and TableFormer. The layout analysis model is an accurate object-detector for page elements, while TableFormer is a state-of-the-art table structure recognition model. These models are provided with pre-trained weights and a separate package for the inference code as docling-ibm-models. They are also used in the open-access deepsearch-experience, a cloud-native service for knowledge exploration tasks.\n",
      "\n",
      "Sources:\n",
      "- text: 'As part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.'\n",
      "  file: 2408.09869v5.pdf\n",
      "  section: 3.2 AI models\n",
      "  page: 3, bounding box: [107, 406, 504, 330]\n",
      "- text: 'Docling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass ( BaseModelPipeline ) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements.\\nImplementations of model classes must satisfy the python Callable interface. The __call__ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly.'\n",
      "  file: 2408.09869v5.pdf\n",
      "  section: 3.4 Extensibility\n",
      "  page: 4, bounding box: [107, 397, 504, 311]\n",
      "- text: 'Docling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown.'\n",
      "  file: 2408.09869v5.pdf\n",
      "  section: 3 Processing pipeline\n",
      "  page: 2, bounding box: [107, 273, 504, 176]\n"
     ]
    }
   ],
   "source": [
    "from docling.chunking import DocChunk\n",
    "\n",
    "print(f\"Question:\\n{QUESTION}\\n\")\n",
    "print(f\"Answer:\\n{rag_res['answer_builder']['answers'][0].data.strip()}\\n\")\n",
    "print(\"Sources:\")\n",
    "sources = rag_res[\"answer_builder\"][\"answers\"][0].documents\n",
    "for source in sources:\n",
    "    doc_chunk = DocChunk.model_validate(source.meta[\"dl_meta\"])\n",
    "    print(f\"- text: {repr(doc_chunk.text)}\")\n",
    "    if doc_chunk.meta.origin:\n",
    "        print(f\"  file: {doc_chunk.meta.origin.filename}\")\n",
    "    if doc_chunk.meta.headings:\n",
    "        print(f\"  section: {' / '.join(doc_chunk.meta.headings)}\")\n",
    "    bbox = doc_chunk.meta.doc_items[0].prov[0].bbox\n",
    "    print(\n",
    "        f\"  page: {doc_chunk.meta.doc_items[0].prov[0].page_no}, \"\n",
    "        f\"bounding box: [{int(bbox.l)}, {int(bbox.t)}, {int(bbox.r)}, {int(bbox.b)}]\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
